
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BMI203:Project 3 Neural Nets &#8212; BMI203:Project 3 Neural Nets  documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="bmi203-project-3-neural-nets">
<h1>BMI203:Project 3 Neural Nets<a class="headerlink" href="#bmi203-project-3-neural-nets" title="Permalink to this headline">¶</a></h1>
<p>Ya’ll were done *\(^o^)/*</p>
<hr class="docutils" />
<span class="target" id="module-Project3.scripts.NN"></span><dl class="py class">
<dt id="Project3.scripts.NN.NeuralNetwork">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">Project3.scripts.NN.</span></code><code class="sig-name descname"><span class="pre">NeuralNetwork</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">setup_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(8,</span> <span class="pre">3,</span> <span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sigmoid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Neural Network class build for simple predictions using one hidden layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>setup_nodes</strong> (<em>np array</em>) – default (8,3,8). The number of nodes in each of the three layers (input, hidden, output). The input layer much match the dimensions of the expected input data used in fit(). The program is not adpated for multiple hidden layers yet.</p></li>
<li><p><strong>activation</strong> (<em>bool</em>) – default “sigmoid”. Activation funtion for all layers. options are “sigmoid”, “relu”, “tanh”</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – default 1. Seed for random weight initialization to ensure reproducible values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>initialized network (of class Neural Network)</p>
</dd>
</dl>
<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.activation_fnct">
<code class="sig-name descname"><span class="pre">activation_fnct</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.activation_fnct" title="Permalink to this definition">¶</a></dt>
<dd><p>Activation functions.</p>
<div class="line-block">
<div class="line">Function takes layer x, a matrix, and calculates the activation function. It will call “self.activation_method” to select which calculation. Current options are sigmoid, relu, tanh</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np array</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.backprop">
<code class="sig-name descname"><span class="pre">backprop</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.backprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Backward propogation of information from output to hidden layers.</p>
<div class="line-block">
<div class="line">This function calculates the current error between the expected ouput (y) and the models current predictions for it (yhat).</div>
<div class="line">Next, the change in layer 3 based on that error is calculated (delta out).</div>
<div class="line">Followed by our new layer 3 weights.</div>
</div>
<div class="line-block">
<div class="line">Given our new output deltas, we can calculate the new hidden deltas and finally the current hidden layer weights.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_data</strong> (<em>np array</em>) – input data to flow through network.</p></li>
<li><p><strong>expected_output</strong> (<em>np array</em>) – expected final output (classes in NN or identical to input_data in autoencoder)</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – default 0.1</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>hidden_weight (np array): Layer2 weights</p></li>
<li><p>output_weight (np array): Layer3 weights</p></li>
<li><p>delta_hidden (np array) : Layer2 errors</p></li>
<li><p>delta_out (np array): Layer3 errors</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.cost_function">
<code class="sig-name descname"><span class="pre">cost_function</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expected_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSE'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.cost_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate cost of descent via loss function.</p>
<div class="line-block">
<div class="line">This function addresses how we minimize our cost function, J. It is called by the fit() function.</div>
<div class="line">For all of our expected training values, calculate the error between true (y) and our current predictons (yhat)</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expected_output</strong> (<em>np array</em>) – expected output of network.</p></li>
<li><p><strong>task</strong> (<em>str</em>) – default “SSE”. Selected loss function. Options are “SSE”: sum of squared errors, “MSE”: mean squared error. (and hopefully binary cross entropy coming soon. )</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>error (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.derivative_fnct">
<code class="sig-name descname"><span class="pre">derivative_fnct</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.derivative_fnct" title="Permalink to this definition">¶</a></dt>
<dd><p>Derivative of the activation functions.</p>
<div class="line-block">
<div class="line">The function calculates the slope of the neuron’s output value. Similarly calls self.activation_method</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>np array</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.feedforward">
<code class="sig-name descname"><span class="pre">feedforward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">report</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.feedforward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass through the network.</p>
<div class="line-block">
<div class="line">This function calculates the hidden_z layer (L2), hidden_activation (A2), output_z (L3), and output_activation (yhat)</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_data</strong> (<em>np array</em>) – input data to flow through network.</p></li>
<li><p><strong>report</strong> (<em>bool</em>) – default False. Used for testing, will output all values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>current predictions for output layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit/Train the model with inputs and hyperparameters.</p>
<div class="line-block">
<div class="line">Train the model based on the input data to learn the expected output. The function employs batch gradient descent as an optimizer.</div>
<div class="line">Training will stop when number of epochs is reached or change in error between epochs is less than cc</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_data</strong> (<em>np array</em>) – input data to flow through network.</p></li>
<li><p><strong>expected_output</strong> (<em>np array</em>) – expected final output (classes in NN or identical to input_data in autoencoder)</p></li>
<li><p><strong>epochs</strong> (<em>int</em>) – default 10. Number of epochs to run in batch gradient descent</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – default 0.01. Learning rate of gradient descent</p></li>
<li><p><strong>loss_function</strong> (<em>str</em>) – default “SSE”. Choice of loss function to optimizer; current options are sum of squared error (SSE) and mean squared error (MSE)</p></li>
<li><p><strong>cc</strong> (<em>float</em>) – default 0.0001 – convergence criteria. When change in error is less than this value, stop iterating.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – default False. If True, will iteratively print the error per epoch.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.init_matrices_to_zero">
<code class="sig-name descname"><span class="pre">init_matrices_to_zero</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.init_matrices_to_zero" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize gradients to zero.</p>
<div class="line-block">
<div class="line">At the begining of every new epoch in our gradient descent we need to start from zeros. This is a helper function to create those initial matrices.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>all np arrays of zeros in differening sizes.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>W1, W2, b1, b2 (np arrays)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.predict">
<code class="sig-name descname"><span class="pre">predict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict output layer for new inputs</p>
<div class="line-block">
<div class="line">With the provided input data, what does the model predict as the output.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_data</strong> (<em>np array</em>) – input data to flow through network. This should be new to network and not the training data ideally.</p></li>
<li><p><strong>task</strong> (<em>str</em>) – default None. If None, will return the actual output layer predictions. If “round”, will return the values rounded to the nearest integer. This is helpful in binary class predictions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output predictions in a matrix the same size as input_data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>outputs (np array)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="Project3.scripts.NN.NeuralNetwork.update_weights">
<code class="sig-name descname"><span class="pre">update_weights</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dW1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dW2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">db1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">db2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.NeuralNetwork.update_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Update layer weights.</p>
<div class="line-block">
<div class="line">This function will calculate the new weights and bias values given the current neuron deltas and our optimizer parameters (so far just learning rate, I didn’t add momentum)</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dW1</strong> (<em>np array</em>) – gradient of hidden weights</p></li>
<li><p><strong>dW2</strong> (<em>np array</em>) – gradient of output weights</p></li>
<li><p><strong>db1</strong> (<em>np array</em>) – gradient of hidden biases</p></li>
<li><p><strong>db2</strong> (<em>np array</em>) – gradient of output biases</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – how should we traverse this gradient</p></li>
<li><p><strong>m</strong> (<em>int</em>) – number of entries in our input (aka 1/m in front of the summations)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>error (float)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="Project3.scripts.NN.check_negatives">
<code class="sig-prename descclassname"><span class="pre">Project3.scripts.NN.</span></code><code class="sig-name descname"><span class="pre">check_negatives</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">positives</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negatives</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.check_negatives" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if any negative sequences appear in positives.</p>
<div class="line-block">
<div class="line">Function returns “NEED TO REMOVE” if there is an error (this has yet to happen)       or “Good to go” if all values are unique.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>positives</strong> (<em>np array</em>) – np array of either int for float</p></li>
<li><p><strong>negatives</strong> (<em>np array</em>) – np array of either int for float</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="Project3.scripts.NN.oneHotDNA">
<code class="sig-prename descclassname"><span class="pre">Project3.scripts.NN.</span></code><code class="sig-name descname"><span class="pre">oneHotDNA</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.oneHotDNA" title="Permalink to this definition">¶</a></dt>
<dd><p>One hot encoding for DNA sequences as a four bit vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq</strong> (<em>str</em>) – DNA sequence input</p></li>
<li><p><strong>flatten</strong> (<em>bool</em>) – whether to flatten to 1D structure or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="Project3.scripts.NN.process_negatives">
<code class="sig-prename descclassname"><span class="pre">Project3.scripts.NN.</span></code><code class="sig-name descname"><span class="pre">process_negatives</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fastafile</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">len_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">17</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">record</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.process_negatives" title="Permalink to this definition">¶</a></dt>
<dd><p>Processing of input negative yeast sequence</p>
<div class="line-block">
<div class="line">This function will read in provided fasta file, optionally downsampmle entries, select a random start site within the selected sequence, and extend to len_k.</div>
<div class="line">The final DNA subsequence will be transformed into a 1D hot-encoded representation.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fastafile</strong> (<em>str</em>) – input fasta file to parse</p></li>
<li><p><strong>downsample</strong> (<em>int</em>) – default None. How many input DNA sequences to randomly select</p></li>
<li><p><strong>len_k</strong> (<em>int</em>) – length of random subsequence to pull from DNA seq</p></li>
<li><p><strong>record</strong> (<em>bool</em>) – default True. Whether to return which DNA sequences were used as input and the random start sites.</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – default 10. random seed for reproducibility</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="Project3.scripts.NN.process_positives">
<code class="sig-prename descclassname"><span class="pre">Project3.scripts.NN.</span></code><code class="sig-name descname"><span class="pre">process_positives</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">NL_delim_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'simple'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(3,</span> <span class="pre">8,</span> <span class="pre">4)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Project3.scripts.NN.process_positives" title="Permalink to this definition">¶</a></dt>
<dd><p>Processing of input positive yeast sequence</p>
<div class="line-block">
<div class="line">This function will read in provided newline delimited file (with no header) and transform all DNA sequences into a 1D hot-encoded representation.</div>
<div class="line">In method “simple” this will take the entire length provided.</div>
<div class="line">In method “sliding” this will generate a sliding window of specified length over every entry prior to 1D transformation.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>NL_delim_file</strong> (<em>str</em>) – new line delimited file to read in of sequnences.</p></li>
<li><p><strong>method</strong> (<em>str</em>) – default (“simple”). “simple” will process all inputs lines as it. “sliding” will enact a sliding window</p></li>
<li><p><strong>window</strong> (<em>np array</em>) – default ((3,8,4)). Enacted if method == “sliding”. Follows the format (No total pieces, final length, overlap length)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
<div class="section" id="indices-and-tables">
<h2>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">BMI203:Project 3 Neural Nets</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Amanda Everitt.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>